http://www.mnemstudio.org/path-finding-q-learning-tutorial.htm

This week's discussion is based on the following questions:
Your understanding of the lecture?
Your thoughts on using RL in your current project, work or a problem of interest?
What is Q- learning?
Run the python code of q-learning and report the results of your experiment on a simple grid world (link to code provided in lecture)


The lecture as I understand it is basically a primer on training-based algorithms and how learning agents interact with rewards and training data to build optimal heuristic sets or goal states. It defines policies based on the reward structure provided, and returns those as the instruction set to follow for that type of work. These learning agents can return the heuristics for a specific function (such as PID or search heuristic tuning) which can then be applied, or they can be used "online" to refine the way an agent chooses the best step forward.

Machine learning is one of my favorite topics, so the more machine learning methods I can use with any given task I will. In this case, RL algorithms seem like a better method for developing acceptable grasps than the trial and error I ended up with. I was looking at using gradient descent, but RL algorithms seem better designed to handle the numerous variables that need to be taken into account when determining acceptable grasp poses, rather than the brute online gradient descent.

Q- learning is an unsupervised RL algorithm based on states and actions. The states could easily be seen as points, and actions move the agent between states. In Q- learning, a reward table is defined which shows which states are accessible, and which states have rewards. If the agent performs an action that reaches a reward state, then the agent knows that is a positive action to take. With a defined reward matrix and an empty trained matrix or Q-matrix, the agent will go through each episode at a random initial state, and then try to get to the goal state where all the rewards are.

The python code from http://firsttimeprogrammer.blogspot.com/2016/09/getting-ai-smarter-with-q-learning.html seems fairly straight forward. The number of paths and size of grid means that finding optimal solutions is fairly straightword, and the results of changing the learning rate wont change the results when there are 10,000 iterations of learning. However, the agent will never settle on a single solution from the starting point 2, as there are two "best" solutions to get to the end of the maze.